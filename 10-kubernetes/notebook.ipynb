{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5e8837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6aba4-5166-4cf9-b7f8-360fdb1f8450",
   "metadata": {},
   "source": [
    "# TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857416c-4513-43fe-b08b-c88bffd9d5bd",
   "metadata": {},
   "source": [
    "The project involves the deployment of a machine learning model using TensorFlow Serving and a Flask-based gateway service. The application architecture comprises two main components: a Docker container housing TensorFlow Serving for serving the machine learning model, and a Flask application acting as the gateway service.\n",
    "\n",
    "The deployment process is outlined in two phases. First, the application is configured to run locally using `Docker Compose`, allowing for easy testing and development. Subsequently, the project introduces the utilization of `Kubernetes` for deploying both the TensorFlow Serving model and the Flask gateway application. Kubernetes, a container orchestration platform, provides a scalable and robust environment for managing and running containerized applications in a production or cloud environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad0d4f-315a-494d-aafc-3e4963284456",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdb0e9b-2781-4f43-b9e3-d6de286f0e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/chapter7-model/xception_v4_large_08_0.894.h5 -O clothing-model-v4.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139947e4-1705-49f9-9c69-907bc3d42b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4b815-ba67-4fab-85d0-a03a28970d7c",
   "metadata": {},
   "source": [
    "To build the app we need to convert the keras model HDF5 into special format called tensorflow SavedModel. For that we download prebuild model and saved it in the working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156b7909-5026-4041-9ba0-997843763f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: clothing-model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: clothing-model\\assets\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('./clothing-model-v4.h5')\n",
    "\n",
    "tf.saved_model.save(model, 'clothing-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97523220-e2d3-4307-86de-227b758bcfdb",
   "metadata": {},
   "source": [
    "Explore the converted model:\n",
    "\n",
    "```bash\n",
    "$ tree clothing model\n",
    "\n",
    "clothing model\n",
    "┣╸ assets\n",
    "┣╸ saved_model.pb\n",
    "┗╸ variables\n",
    "    ┣╸ variables.data-00000-of-00001\n",
    "    ┗╸ variables.index\n",
    "```\n",
    "\n",
    "```bash\n",
    "$ ls -lhR clothing-model\n",
    "clothing-model:\n",
    "total 2.8M\n",
    "\n",
    "clothing-model/assets:\n",
    "total 0\n",
    "\n",
    "clothing-model/variables:\n",
    "total 83M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee3034-7fcc-4cf0-8ef5-bd1e7980775a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. TensorFlow Serving with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec093a-6695-49c8-a01a-a5bd486c6058",
   "metadata": {},
   "source": [
    "We can look what's inside in the saved model using the utility (`saved_model_cli`) from tensorflow and the command `saved_model_cli show --dir model-dir-name --all`. Running the command outputs few things but we are interested in the signature, specifically `signature_def['serving_default']`, which shows the inputs and outputs of the model. For instance, the example model has 1 input ( `input_8`) and 1 output (`dense_7`).    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e3f8b-c2ca-40a6-a864-f71277151e64",
   "metadata": {
    "tags": []
   },
   "source": [
    "```bash\n",
    "$ saved_model_cli show --dir clothing-model --all\n",
    "\n",
    "signature_def['__saved_model_init_op']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['__saved_model_init_op'] tensor_info:\n",
    "        dtype: DT_INVALID\n",
    "        shape: unknown_rank\n",
    "        name: NoOp\n",
    "  Method name is: \n",
    "\n",
    "signature_def['serving_default']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['input_8'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 299, 299, 3)\n",
    "        name: serving_default_input_8:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['dense_7'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 10)\n",
    "        name: StatefulPartitionedCall:0\n",
    "  Method name is: tensorflow/serving/predict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfec374-08d1-4115-9039-17c88b3b15f3",
   "metadata": {},
   "source": [
    "We can run the model (`clothing-model`) with the prebuilt docker image `tensorflow/serving:2.7.0`:\n",
    "\n",
    "```bash\n",
    "$ docker run -it --rm \\\n",
    "  -p 8500:8500 \\\n",
    "  -v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
    "  -e MODEL_NAME=\"clothing-model\" \\\n",
    "  tensorflow/serving:2.7.0\n",
    "```\n",
    "\n",
    "* docker run -it --rm (to run the docker)\n",
    "* -p 8500:8500 (port mapping)\n",
    "* -v $(pwd)/clothing-model:/models/clothing-model/1 (volume mapping of absolute model directory to model directory inside the docker image. \"models/model_name/version\")\n",
    "* -e MODEL_NAME=\"clothing-model\" (set environment variable for docker image)\n",
    "* tensorflow/serving:2.7.0 (name of the image to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec215ce-52a7-4112-b8a0-4eb2441f67bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Establish gRPC Connection to TensorFlow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1588cca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e4788a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install keras-image-helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a456a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import grpc\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "\n",
    "from keras_image_helper import create_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2f652-1113-4f32-b35a-6248d5d41e9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The stub is a client-side object that allows the code to make remote calls to the `PredictionService` using the specified communication channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09c215c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "host = 'localhost:8500'\n",
    "\n",
    "channel = grpc.insecure_channel(host)\n",
    "\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbaa03-1d82-4b95-8dea-a5525e5c706b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Prediction Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77766e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the input image \n",
    "preprocessor = create_preprocessor('xception', target_size=(299, 299))\n",
    "\n",
    "url = 'http://bit.ly/mlbookcamp-pants'\n",
    "X = preprocessor.from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f0c4b-5a09-4f84-abb1-e396f7acc665",
   "metadata": {},
   "source": [
    "Tensorflow uses specical serving called `gRPC` protocol which is optimized to use binary data format. We need to convert the inputs for our prediction request into `protobuf` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9810d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def np_to_protobuf(data):\n",
    "    return tf.make_tensor_proto(data, shape=data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ce47b-fa18-445a-9640-ce2a8771bd01",
   "metadata": {
    "tags": []
   },
   "source": [
    "np_to_protobuf(X)\n",
    "```\n",
    "dtype: DT_FLOAT\n",
    "tensor_shape {\n",
    "  dim {\n",
    "    size: 1\n",
    "  }\n",
    "  dim {\n",
    "    size: 299\n",
    "  }\n",
    "  dim {\n",
    "    size: 299\n",
    "  }\n",
    "  dim {\n",
    "    size: 3\n",
    "  }\n",
    "}\n",
    "tensor_content: \"\\350\\350\\350...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7dbcd8-d619-47f0-af99-71c259f5bfbb",
   "metadata": {},
   "source": [
    "`PredictRequest` is used to encapsulate the input data or parameters (model name, model's signature name, and input in `protobuf` format) that the client wants to send to the server when making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3659a577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pb_request = predict_pb2.PredictRequest()\n",
    "\n",
    "pb_request.model_spec.name = 'clothing-model'\n",
    "pb_request.model_spec.signature_name = 'serving_default'\n",
    "\n",
    "# Set the model's input (\"input_8\")\n",
    "pb_request.inputs['input_8'].CopyFrom(np_to_protobuf(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c2bc0-db75-4087-9df4-f473a39582b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "pb_request\n",
    "\n",
    "```\n",
    "model_spec {\n",
    "  name: \"clothing-model\"\n",
    "  signature_name: \"serving_default\"\n",
    "}\n",
    "inputs {\n",
    "  key: \"input_8\"\n",
    "  value {\n",
    "    dtype: DT_FLOAT\n",
    "    tensor_shape {\n",
    "      dim {\n",
    "        size: 1\n",
    "      }\n",
    "      dim {\n",
    "        size: 299\n",
    "      }\n",
    "      dim {\n",
    "        size: 299\n",
    "      }\n",
    "      dim {\n",
    "        size: 3\n",
    "      }\n",
    "    }\n",
    "    tensor_content: \"\\350\\350...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32578c96-96df-43ec-b346-7a28af99a909",
   "metadata": {},
   "source": [
    "### Make Prediction Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b02eb9-415c-4248-a985-68994bb1ab47",
   "metadata": {},
   "source": [
    "The `Predict()` method returns a response object with a value in `Protobuf` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6823df3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pb_response = stub.Predict(pb_request, timeout=20.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb14248-d673-4193-8265-2d670520c0e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "pb_response\n",
    "\n",
    "```\n",
    "outputs {\n",
    "  key: \"dense_7\"\n",
    "  value {\n",
    "    dtype: DT_FLOAT\n",
    "    tensor_shape {\n",
    "      dim {\n",
    "        size: 1\n",
    "      }\n",
    "      dim {\n",
    "        size: 10\n",
    "      }\n",
    "    }\n",
    "    float_val: -1.8682905435562134\n",
    "    float_val: -4.761244297027588\n",
    "    float_val: -2.316984176635742\n",
    "    float_val: -1.0625706911087036\n",
    "    float_val: 9.887160301208496\n",
    "    float_val: -2.812433958053589\n",
    "    float_val: -3.6662838459014893\n",
    "    float_val: 3.200362205505371\n",
    "    float_val: -2.6023383140563965\n",
    "    float_val: -4.835046768188477\n",
    "  }\n",
    "}\n",
    "model_spec {\n",
    "  name: \"clothing-model\"\n",
    "  version {\n",
    "    value: 1\n",
    "  }\n",
    "  signature_name: \"serving_default\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43a0ef-f6b2-4ad3-8f72-084c940a2d9f",
   "metadata": {},
   "source": [
    "### Retrieve and Interpret Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0dece29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.8682905435562134, -4.761244297027588, -2.316984176635742, -1.0625706911087036, 9.887160301208496, -2.812433958053589, -3.6662838459014893, 3.200362205505371, -2.6023383140563965, -4.835046768188477]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model's output \"dense_7\" and read response\n",
    "preds = pb_response.outputs['dense_7'].float_val\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5b392f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a77fb533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -1.8682905435562134,\n",
       " 'hat': -4.761244297027588,\n",
       " 'longsleeve': -2.316984176635742,\n",
       " 'outwear': -1.0625706911087036,\n",
       " 'pants': 9.887160301208496,\n",
       " 'shirt': -2.812433958053589,\n",
       " 'shoes': -3.6662838459014893,\n",
       " 'shorts': 3.200362205505371,\n",
       " 'skirt': -2.6023383140563965,\n",
       " 't-shirt': -4.835046768188477}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(classes, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9f660-90ff-4002-b5e6-97574a535e03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Creating a gateway service with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b00a37-0816-4224-be2f-83d2aff34ae6",
   "metadata": {},
   "source": [
    "The application consists of two components: a `Docker container` with Tensorflow Serving and a `Flask application` with the gateway service (which will be dockerized in the next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c09a8-3363-4e25-aabe-73104ed43c95",
   "metadata": {},
   "source": [
    "Convert the code of the previous section into a script and rename it as `gateway.py`. \n",
    "\n",
    "```bash\n",
    "$ jupyter nbconvert --to script notebook.ipynb\n",
    "```\n",
    "\n",
    "Create functions to prepare request, send request, and prepare response. Convert the script into a Flask app:\n",
    "\n",
    "```python\n",
    "app = Flask('gateway')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_endpoint():\n",
    "    data = request.get_json()\n",
    "    url = data['url']\n",
    "    result = predict(url)\n",
    "    return jsonify(result)\n",
    "```\n",
    "\n",
    "Since Tensorflow is a large library, we can use the `proto.py` script instead to convert numpy array into protobuf format. Import the `np_to_protobuf` function into our `gateway.py` script. For this, we will need to install `tensorflow-protobuf==2.7.0` ([tensorflow-protobuf](https://github.com/alexeygrigorev/tensorflow-protobuf)) and `protobuf==3.19`.\n",
    "\n",
    "We also want to put everything in the `pipenv` for deployment. Install the following libraries: \n",
    "\n",
    "```bash\n",
    "$ pipenv install grpcio==1.42.0 flask gunicorn keras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4182c-ec2e-4e66-aaf7-3adafdba7a77",
   "metadata": {},
   "source": [
    "Run the model using the prebuilt Docker image with TensorFlow Serving, as explained in the previous section, and the Flask app `gateway.py` within the Pipenv environment:\n",
    "\n",
    "```bash\n",
    "$ pipvenv shell\n",
    "$ python gateway.py\n",
    "```\n",
    "\n",
    "Test the gateway with the `test.py` script:\n",
    "\n",
    "```bash\n",
    "$ python test.py \n",
    "{'dress': -1.8798640966415405, 'hat': -4.75631046295166, 'longsleeve': -2.359532356262207, 'outwear': -1.0892651081085205, 'pants': 9.903783798217773, 'shirt': -2.8261783123016357, 'shoes': -3.6483113765716553, 'shorts': 3.2411551475524902, 'skirt': -2.612095832824707, 't-shirt': -4.852035045623779}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6bd9fa-d2a0-4843-bbd8-69786cb51e2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Running app locally with Docker Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ffd50-2d77-4b9b-bde8-73d6521ddc6f",
   "metadata": {},
   "source": [
    "In this section, we will dockerize the `gateway` service and run it locally alongside the `TensorFlow Serving` container.\n",
    "\n",
    "First, we can use the `image-model.dockerfile` template to build a new image based on the prebuilt `tensorflow/serving:2.7.0` image:\n",
    "\n",
    "```bash\n",
    "$ docker build -t clothing-model:xception-v4-001 -f image-model.dockerfile .\n",
    "$ docker run -it --rm -p 8500:8500 clothing-model:xception-v4-001\n",
    "```\n",
    "\n",
    "Second, in a similar manner, we can dockerize the `gateway` service using the `image-gateway.dockerfile` template. Previously, we need to update the host variable in the `gateway.py` script, so it can read the environment variable 'TF_SERVING_HOST': \n",
    "\n",
    "```python\n",
    "host = os.getenv('TF_SERVING_HOST', 'localhost:8500')\n",
    "```\n",
    "\n",
    "```bash\n",
    "$ docker build -t clothing-model-gateway:001 -f image-gateway.dockerfile .\n",
    "$ docker run -it --rm -p 9696:9696 clothing-model-gateway:001\n",
    "```\n",
    "\n",
    "With both containers running, we can test the app (running `test.py`), however it will fail (`status = StatusCode.UNAVAILABLE` `details = \"failed to connect to all addresses\"`) since the `gateway` service is unable to connect to the model in the `TensorFlow Serving` container. To address this issue and establish a connection between both containers, we can use Docker Compose. \n",
    "\n",
    "We can create a YAML file (`docker-compose.yaml`) to define both services and run them locally in the same environment with one single command:\n",
    "\n",
    "```bash\n",
    "$ docker-compose up\n",
    "```\n",
    "\n",
    "Useful Docker Compose commands:\n",
    "* `docker-compose up`:  run docker compose\n",
    "* `docker-compose up -d`:  run docker compose in detached mode\n",
    "* `docker ps`:  to see the running containers\n",
    "* `docker-compose down`: stop the docker compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8515d-1fe3-4d4c-95e8-0cd135f8161e",
   "metadata": {},
   "source": [
    "## 5. Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe596b-ecc6-4f27-9e80-51a6e3483800",
   "metadata": {},
   "source": [
    "Kubernetes (K8s) is open source system for automating deployment, scaling and management of containerized applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6278b0f3-1ddd-488a-8d6d-b03c87d5e24d",
   "metadata": {},
   "source": [
    "Kubernetes components:\n",
    "* Cluster\n",
    "* Node\n",
    "* Pod\n",
    "* Deployment\n",
    "* Service\n",
    "* Ingress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4cfaf-e8d6-433a-a20f-2d538f94f28d",
   "metadata": {},
   "source": [
    "## 6. Deploying a simple service to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b8ce9-d47f-40b3-9eb7-000e97008b6c",
   "metadata": {},
   "source": [
    "**Create a Flask app**  \n",
    "\n",
    "* Create a simple Flask app: `ping.py`\n",
    "* Install libraries: ```$ pipenv flask gunicorn```\n",
    "* Create a Dockerfile.  \n",
    "* Build a Docker image: ```$ docker build -t ping:v001 .```\n",
    "* Run container: ```$ docker run -it --rm -p 9696:9696 ping:v001```\n",
    "* Test app: ```$ curl localhost:9696/ping```\n",
    "* Stop container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd132396-8a6e-4203-bb67-e78ec1afc644",
   "metadata": {},
   "source": [
    "**Create a Kubernetes cluster**  \n",
    "\n",
    "* Install `kubectl` from [Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html). [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) is the Kubernetes command-line tool to deploy applications, inspect and manage cluster resources, and view logs. We can also use the `kubectl` version included with Docker Desktop.  \n",
    "Note from AWS:  \n",
    "You must use a kubectl version that is within one minor version difference of your Amazon EKS cluster control plane. For example, a 1.27 kubectl client works with Kubernetes 1.26, 1.27, and 1.28 clusters.  \n",
    "Run ```$ kubectl version --client``` verify kubectl version.  \n",
    "\n",
    "* Install `Kind` to set up a local Kubernetes cluster. We can install Kind from [release binaries](https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries).\n",
    "\n",
    "* Create and run a cluster:  \n",
    "    * ```$ kind create cluster```: create cluster\n",
    "    * ```$ kubectl cluster-info --context kind-kind```: read cluster info with kubectl\n",
    "    * ```$ kubectl get service```: read service info (default: kubernetes service)\n",
    "    * ```$ docker ps```: inspect the container running the `kindest` image.  \n",
    "\n",
    "* Useful commands:  \n",
    "    * ```$ kubectl get pod```\n",
    "    * ```$ kubectl get deployment```\n",
    "    * ```$ kind delete cluster```  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b89337-6fa7-426c-8398-91034809fa1f",
   "metadata": {},
   "source": [
    "**Create a deployment**  \n",
    "It is a resource that manages the deployment and scaling of a set of pods.\n",
    "\n",
    "* Install the [Kubernetes](https://marketplace.visualstudio.com/items?itemName=ms-kubernetes-tools.vscode-kubernetes-tools) extension for VS Code.  \n",
    "* Create the `deployment.yaml` file. Type \"deployment\" to autocomplete a template and update the file.  \n",
    "    \n",
    "    `replicas: 1`: Specifies that the desired number of replicas (instances) of the application should be 1.  \n",
    "    `selector`: Describes how the Deployment identifies which pods to manage. `matchLabels`: Specifies a set of key-value pairs. Pods managed by this Deployment should have labels matching these values. In this case,`app: ping`.  \n",
    "    `template`: Describes the pod template that will be used to create new pods.  \n",
    "    `metadata`: Provides labels for the pods created from this template. The label `app: ping` is assigned.  \n",
    "    `spec`: Describes the pod's specification, including the containers running within the pod. `containers`: Describes the containers within the pod.\n",
    "        \n",
    "* Apply the deployment to the Kubernetes cluster: ```$ kubectl apply -f deployment.yaml``` (to remove a deployment run: ```$ kubectl delete -f deployment.yaml```)  \n",
    "\n",
    "* Read pod status:  \n",
    "    * ```$ kubectl get pod```\n",
    "    * ```$ kubectl describe pod ping-deployment-<complete-pod-name>```  \n",
    "\n",
    "* Load the Docker image with the Flask app into the cluster: ```$ kind load docker-image ping:v001```\n",
    "* Use Port Forwarding to access applications in a cluster: ```$ kubectl port-forward ping-deployment-<complete-pod-name> 9696:9696```  \n",
    "* Test app: ```$ curl localhost:9696/ping```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d56db0f-5c39-4256-975d-eae4e75c624f",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Create a service**   \n",
    "A Service is a method for exposing a network application that is running as one or more Pods in your cluster.\n",
    "\n",
    "* Create the `service.yaml` file. Type \"service\" to autocomplete a template and update the file.  \n",
    "    `type: LoadBalancer`: Specifies the type of Service to be a LoadBalancer. This is used when you want to expose your service to the external world, and the cloud provider will provision a load balancer to distribute traffic to the service.  \n",
    "    `selector`: Specifies a set of labels to select the pods to expose via the service. In this case, only pods with the label `app: ping` will be part of this service.  \n",
    "    `ports`: Specifies the ports that the service should listen on. Traffic arriving at `port: 80` on the service will be forwarded to `port: 9696` on the selected pods.\n",
    "    \n",
    "* Apply the service: ```$ kubectl apply -f service.yaml```. The `ping` service will be created (check status with ```$ kubectl get service```)\n",
    "\n",
    "* Use Port Forwarding to access the app: ```$ kubectl port-forward service/ping 8080:80```\n",
    "* Test app: ```$ curl localhost:8080/ping```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859fdce4-a864-425b-ad67-00d147cd7359",
   "metadata": {},
   "source": [
    "## 7. Deploying TensorFlow models to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771940bf-bca5-4fc3-a2c6-5877e7d3eec8",
   "metadata": {},
   "source": [
    "Create the `kube-config` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658cb2a-1991-4107-b18a-2cea832fdb1d",
   "metadata": {},
   "source": [
    "**Model**  \n",
    "* Create a deployment for the model: `model-deployment.yaml`  \n",
    "* Load the Docker image with the model into the cluster: ```$ kind load docker-image clothing-model:xception-v4-001```\n",
    "* Apply the deployment ```$ kubectl apply -f model-deployment.yaml```\n",
    "* Check pod status.\n",
    "* Use Port Forwarding to access the app: ```$ kubectl port-forward tf-serving-clothing-model-<complete-pod-name> 8500:8500```\n",
    "* Test app running: ```$ python gateway.py```  \n",
    "* Create a service: `model-service.yaml` (service `type: ClusterIP`)\n",
    "* Apply the service ```$ kubectl apply -f model-service.yaml```\n",
    "* ```$ kubectl port-forward service/tf-serving-clothing-model 8500:8500```\n",
    "* Test app running: ```$ python gateway.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f3c3b-868c-44af-abfd-ca0725b23f00",
   "metadata": {},
   "source": [
    "**Gateway**\n",
    "* Create a deployment for the gateway: `gateway-deployment.yaml`  \n",
    "* Load the Docker image with the gateway into the cluster: ```$ kind load docker-image clothing-model-gateway:001```\n",
    "* Apply the deployment ```$ kubectl apply -f gateway-deployment.yaml```\n",
    "* Create a service: `gateway-service.yaml` (service `type: LoadBalancer`)  \n",
    "* Apply the service ```$ kubectl apply -f gateway-service.yaml```\n",
    "* Use Port Forwarding to access the app: ```$ kubectl port-forward service/gateway 8080:80```\n",
    "* Update the url in  `gateway.py` with port 8080 and run the script to test the app:\n",
    "```bash\n",
    "$ python test.py \n",
    "{'dress': -1.8798640966415405, 'hat': -4.75631046295166, 'longsleeve': -2.359532356262207, 'outwear': -1.0892651081085205, 'pants': 9.903783798217773, 'shirt': -2.8261783123016357, 'shoes': -3.6483113765716553, 'shorts': 3.2411551475524902, 'skirt': -2.612095832824707, 't-shirt': -4.852035045623779}\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1614c4-d0dd-4887-92f0-67ab5eab342f",
   "metadata": {},
   "source": [
    "## 7. Deploying to Amazon EKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82183dec-ec75-4109-bdc8-6d4b3b6e51f1",
   "metadata": {},
   "source": [
    "* Amazon Elastic Kubernetes Service ([Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/setting-up.html)) is a managed Kubernetes service to run Kubernetes in the AWS cloud and on-premises data centers.\n",
    "* Install [eksctl](https://eksctl.io/installation/) which is a simple CLI tool for creating and managing clusters on Amazon EKS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2357e2-abba-490e-aa57-2a1a1b00fa52",
   "metadata": {},
   "source": [
    "**Amazon EKS Cluster**\n",
    "* Create a cluster from the `eks-config.yaml` file: \n",
    "```bash\n",
    "$ eksctl create cluster -f eks-config.yaml\n",
    "kubectl command should work with \"C:\\\\Users\\\\...\", try 'kubectl get nodes'\n",
    "EKS cluster \"mlzoomcamp-eks\" in \"us-east-1\" region is ready\n",
    "```\n",
    "```\n",
    "$ kubectl get nodes\n",
    "$ docker ps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e1c9e-762a-46c8-ae60-55ba5701e0fa",
   "metadata": {},
   "source": [
    "**Amazon ECR repository**\n",
    "* Use the AWS CLI to create an Amazon ECR repository (view lesson 09):\n",
    "\n",
    "```bash\n",
    "$ aws ecr create-repository --repository-name mlzoomcamp-images\n",
    "```\n",
    "\n",
    "* To authenticate your Docker client to your private ECR registry, run the following command:  \n",
    "The ```aws ecr get-login-password``` command retrieves the password required to authenticate to your Amazon ECR registry using the docker login command, so you can pull and push images. Be sure that `<region>` matches the region in your registry URL. Also, update your `<aws_account_id>`. \n",
    "\n",
    "```bash\n",
    "$ aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\n",
    "```\n",
    "\n",
    "* Tag the local images for the model and the gateway before pushing them to the Amazon ECR registry:\n",
    "\n",
    "```bash\n",
    "$ PREFIX=<aws_account_id>.dkr.ecr.<region>.amazonaws.com/mlzoomcamp-images\n",
    "$ TAG=clothing-model-gateway-001\n",
    "$ GATEWAY_REMOTE_URI=${PREFIX}:${TAG}\n",
    "$ echo ${GATEWAY_REMOTE_URI}\n",
    "$ docker tag clothing-model-gateway:001 ${GATEWAY_REMOTE_URI}\n",
    "```\n",
    "\n",
    "```bash\n",
    "$ TAG=clothing-model-xception-v4-001\n",
    "$ MODEL_REMOTE_URI=${PREFIX}:${TAG}\n",
    "$ echo ${MODEL_REMOTE_URI}\n",
    "$ docker tag clothing-model:xception-v4-001 ${MODEL_REMOTE_URI}\n",
    "```\n",
    "\n",
    "* Push the images using the docker push command:\n",
    "```bash\n",
    "$ docker push ${GATEWAY_REMOTE_URI}\n",
    "$ docker push ${MODEL_REMOTE_URI}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c9a83-a0d5-47f2-8a79-fce3a9acbf2b",
   "metadata": {},
   "source": [
    "**Apply deployments and services to EKS to test the app**\n",
    "\n",
    "* Update `model-deployment.yaml` and `gateway-deployment.yaml` with the ECR image URIs.\n",
    "* ```$ kubectl apply -f model-deployment.yaml```\n",
    "* ```$ kubectl apply -f model-service.yaml```\n",
    "* ```$ kubectl apply -f gateway-deployment.yaml```\n",
    "* ```$ kubectl apply -f gateway-service.yaml```\n",
    "* ```$ kubectl get pod```\n",
    "* Get the external IP (EC2 load balancer DNS name) for the gateway service: ```$ kubectl get service```\n",
    "* Use Port Forwarding to access the app: ```$ kubectl port-forward service/gateway 8080:80```\n",
    "* Update the url in `gateway.py` with the external ip for the gateway service and run the script to test the app:\n",
    "\n",
    "```bash\n",
    "$ python test.py \n",
    "{'dress': -1.8798640966415405, 'hat': -4.75631046295166, 'longsleeve': -2.359531879425049, 'outwear': -1.0892632007598877, 'pants': 9.90378189086914, 'shirt': -2.8261773586273193, 'shoes': -3.6483097076416016, 'shorts': 3.241151809692383, 'skirt': -2.6120948791503906, 't-shirt': -4.852035999298096}\n",
    "(ml-zoomcamp) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbebb50-1509-4a80-bddd-e38e3bf16399",
   "metadata": {},
   "source": [
    "* Delete the EKS cluster:\n",
    "```bash\n",
    "$ eksctl delete cluster --name mlzoomcamp-eks\n",
    "will delete stack \"eksctl-mlzoomcamp-eks-cluster\"\n",
    "all cluster resources were deleted\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
